{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章 : EMアルゴリズム\n",
    "## KLダイバージェンス\n",
    "### 数式の表記について\n",
    "以下、確率変数$x$の確率密度が$p\\left(x\\right)$で表されるときの$f\\left(x\\right)$の期待値を\n",
    "$$\n",
    "\\mathbb{E}_{p\\left(x\\right)} [ f\\left(x\\right)] := \\int{f\\left(x\\right)p\\left(x\\right)dx}\n",
    "$$\n",
    "とかく。また、$p\\left(x;\\theta\\right)をp_{\\theta}\\left(x\\right)$とかく\n",
    "\n",
    "\n",
    "### KLダイバージェンスとは\n",
    "2つの確率分布を測る尺度に用いる。確率分布$p\\left(x\\right),q\\left(x\\right)$が与えられたときのKLダイバージェンスは、\n",
    "$$\n",
    "D_{KL} \\left(p \\parallel q \\right) = \\int p\\left(x\\right) \\log{\\frac{p\\left(x\\right)}{q\\left(x\\right)}dx}\n",
    "$$\n",
    "である。xが離散変数の場合は、\n",
    "$$\n",
    "D_{KL} \\left(p \\parallel q \\right) = \\sum p\\left(x\\right) \\log{\\frac{p\\left(x\\right)}{q\\left(x\\right)}dx}\n",
    "$$\n",
    "である。\n",
    "\n",
    "このKLダイバージェンスには次の特性がある\n",
    "- 2つの確率分布が異なるほど大きい値をとる\n",
    "- 0以上の値をとり、2つの確率分布が同じ時のみ0になる\n",
    "- 非対称な尺度であるため、$D_{KL}\\left(p \\parallel q \\right) \\neq D_{KL}\\left(q \\parallel p \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KLダイバージェンスと最尤推定の関係\n",
    "真の確率分布$p_*\\left(x\\right)$があり、サンプルデータ$\\{x^{\\left(1\\right)},x^{\\left(2\\right)},\\cdots,x^{\\left(N\\right)}\\}$を生成したとき、対数尤度は\n",
    "\n",
    "$$\n",
    "\\log{\\prod_{n=1}^{N}{p_{\\theta}\\left(x^{\\left(n\\right)}\\right)}} = \\sum_{n=1}^{N}\\log p_{\\theta}\\left(x^{\\left(n\\right)}\\right)\n",
    "$$\n",
    "\n",
    "とかけて、対数尤度を最大化するパラメータ$\\theta$は\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\argmax_{\\theta}\\sum_{n=1}^{N}{\\log p_{\\theta}\\left(x^{\\left(n\\right)}\\right)}\n",
    "$$\n",
    "\n",
    "である"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、この最尤推定の式をKLダイバージェンスを使って導出する\n",
    "\n",
    "最尤推定の目的はパラメータ$\\theta$を変更することで$p_{\\theta}\\left(x\\right)$を真の確率分布$p_*\\left(x\\right)$に近づけることであり、これは次のKLダイバージェンスの最小化であると言える\n",
    "\n",
    "$$\n",
    "D_{KL}\\left(p_* \\parallel p_{\\theta} \\right) = \\int p_* \\left(x\\right) \\log \\frac{p_* \\left(x\\right)}{p_{\\theta}\\left(x\\right)dx}\n",
    "$$\n",
    "\n",
    "これは$p_*\\left(x\\right)$が未知であるから計算不可であるが、以下のモンテカルロ(Monte Carlo method)を使って近似する\n",
    "### モンテカルロ法\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{p\\left(x\\right)} [ f\\left(x\\right)] &= \\int{f\\left(x\\right)p\\left(x\\right)dx} \\\\\n",
    "&\\approx \\frac{1}{N} \\sum_{n=1}^{N}{f\\left(x^{\\left(n\\right)}\\right)} \\quad \\left(x^{\\left(n\\right)} \\sim p_*\\left(x\\right)\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "いま、$f\\left(x\\right) = \\log \\frac{p_*\\left(x\\right)}{p_{\\theta}\\left(x\\right)}$であるから、\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}\\left(p_* \\parallel p_{\\theta} \\right) &= \\int p_* \\left(x\\right) \\log \\frac{p_* \\left(x\\right)}{p_{\\theta}\\left(x\\right)dx} \\\\\n",
    "&\\approx \\frac{1}{N}\\sum_{n=1}^{N} \\log {\\frac{p_* \\left(x^{\\left(x\\right)}\\right)}{p_{\\theta}\\left(x^{\\left(n\\right)}\\right)}} \\quad \\left(x^{\\left(n\\right)} \\sim p_*\\left(x\\right)\\right) \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^{N}{\\left(\\log p_*\\left(x^{\\left(n\\right)}\\right) - \\log p_{\\theta}\\left(x^{\\left(n\\right)}\\right) \\right)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "このとき、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\argmin_{\\theta} D_{KL}\\left(p_* \\parallel p_{\\theta}\\right) &\\approx \\argmin_{\\theta} \\left(-\\frac{1}{N}\\sum_{n=1}^{N}{\\log p_{\\theta}\\left(x_n\\right)} \\right) \\\\\n",
    "&= \\argmax_{\\theta}  \\sum_{n=1}^{N}{\\log p_{\\theta}\\left(x_n\\right)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "よって、\n",
    "\n",
    "$$\n",
    "\\argmin_{\\theta} D_{KL} \\left(p_* \\parallel p_{\\theta}\\right) \\approx \\argmax_{\\theta} \\sum_{n=1}^{N}{\\log p_{\\theta} \\left(x_n \\right)}\n",
    "$$\n",
    "\n",
    "となりKLダイバージェンスの最小化から最尤推定に帰着する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMアルゴリズム\n",
    "最尤推定が解析的に解けない潜在変数を持つモデルにたいして使われるものである。(GMMはカテゴリカル分布で$z=k$という潜在変数$z$があるためEMアルゴリズムはGMMにも使える)\n",
    "\n",
    "### 潜在変数をもつモデル\n",
    "潜在変数を持つモデルは、確率変数$x$、潜在変数$z$、パラメータ$\\theta$を用いて\n",
    "$$\n",
    "\\log p_{\\theta}\\left(x \\right) = \\log {\\sum_{z}{p_{\\theta}\\left(x,z\\right)}}\n",
    "$$\n",
    "とかける。連続変数の場合は$\\sum$が$\\int$になるだけである。\n",
    "\n",
    "いま、サンプル$\\mathcal{D} = \\{x^{\\left(1\\right)},x^{\\left(2\\right)},\\cdots,x^{\\left(N\\right)}\\}$が得られた時を考える。このときの対数尤度は\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_{\\theta} \\left( \\mathcal{D} \\right) &= \\log \\left( p \\left( x^{\\left(1\\right)} \\right) , p \\left( x^{\\left(2\\right)} \\right),\\cdots, p \\left( x^{\\left(N\\right)} \\right) \\right) \\\\\n",
    "&= \\sum_{n=1}^{N}{\\log \\sum_{z^{\\left(n\\right)}}{p_{\\theta}\\left(x^{\\left(n\\right)},z^{\\left(n\\right)}\\right)}}\n",
    "\\end{align}\n",
    "$$\n",
    "とかける。前章でやったとおり、この対数尤度の最大化は解析的に解けない。そこで、EMアルゴリズムはこのlog-sumの形をsum-logの形に変換する。\n",
    "\n",
    "まず、$p_{\\theta}\\left(x\\right)$に対して$q\\left(z\\right)$を用いて\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_{\\theta} \\left(x \\right) & \\log \\frac{p_{\\theta}\\left(x, z\\right)}{p_{\\theta}\\left(z | x \\right)} \\\\\n",
    "&= \\log \\frac{p_{\\theta}\\left(x, z\\right)}{p_{\\theta}\\left(z | x \\right)} \\frac{q\\left(z\\right)}{q\\left(z\\right)} \\\\\n",
    "&= \\log \\frac{p_{\\theta}\\left(x,z\\right)}{q\\left(z\\right)} + \\log \\frac{q\\left(z\\right)}{p_{\\theta}\\left(z|x\\right)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と変形する。これを用いて、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_{\\theta}(x) &= \\log p_{\\theta}(x) \\sum_z q(z) \\quad \\left(\\because \\sum_{z}q\\left(z\\right)=1\\right)\\\\\n",
    "&= \\sum_z q(z) \\log p_{\\theta}(x) \\\\\n",
    "&= \\sum_z q(z) \\left( \\log \\frac{p_{\\theta}\\left(x,z\\right)}{q\\left(z\\right)} + \\log \\frac{q\\left(z\\right)}{p_{\\theta}(z|x)} \\right)\\\\\n",
    "&= \\sum_z q(z) \\log \\frac{p_{\\theta}(x,z)}{q(z)} + \\underbrace{\\sum_z q(z) \\log \\frac{q(z)}{p_{\\theta}(z|x)}}_{\\text{KL ダイバージェンス}} \\\\\n",
    "&= \\sum_z q(z) \\log \\frac{p_{\\theta}(x,z)}{q(z)} + D_{KL}(q(z) \\parallel p_{\\theta}(z|x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "とかける"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO\n",
    "ここで、KLダイバージェンスは0以上であるから\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_{\\theta}(x) &= \\sum_z q(z) \\log \\frac{p_{\\theta}(x, z)}{q(z)} + D_{KL}(q(z) \\parallel p_{\\theta}(z|x)) \\\\\n",
    "&\\geq \\sum_z q(z) \\log \\frac{p_{\\theta}(x, z)}{q(z)}\n",
    "\\end{align}\n",
    "$$\n",
    "となる。ここの第一項はELBO(Evidence Lower Bound)(エビデンスの下界)(エビデンスは対数尤度の別名)と呼ばれる。\n",
    "\n",
    "以降、\n",
    "$$\n",
    "{\\rm{ELBO}} \\left(x;q,\\theta\\right) = \\sum_{z}{q\\left(z\\right)}\\log \\frac{p_\\theta\\left(x,z\\right)}{q\\left(z\\right)}\n",
    "$$\n",
    "とする。${\\rm{ELBO}} \\left(x;q,\\theta\\right) $の特徴として以下がある。\n",
    "- 対数尤度の下界\n",
    "- sum-logの形になっていて解析しやすい\n",
    "そのため、対数尤度を最大化するのではなく、このELBOを最大化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMアルゴリズム\n",
    "${\\rm{ELBO}} \\left(x;q,\\theta\\right)$には2つのパラメータ$q\\left(z\\right), \\theta$がある。片方を固定してもう片方のパラメータを固定するということを繰り返して最適解を求める。\n",
    "\n",
    "まず、$\\theta = \\theta_{old}$として$\\theta$をまずは固定する。\n",
    "このとき、\n",
    "$$\n",
    "\\log p_{\\theta}\\left(x\\right) = {\\rm{ELBO}} \\left(x;q,\\theta\\right)  + D_{KL}\\left(q\\left(z\\right) \\parallel p_{\\theta}\\left(z | x \\right)\\right)\n",
    "$$\n",
    "より、$q\\left(z|\\right)$の更新式は$q\\left(z\\right)=p_{\\theta_{old}}\\left(z | x\\right)$とかける。(KLダイバージェンスを0にするのが最適であるため)\n",
    "\n",
    "この更新は、EMアルゴリズムのEステップ(Expectation Value : 期待値)と呼ばれる。由来としては、この更新後のELBOが期待値として表されるからである。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{ELBO}(x; q = p_{\\mathrm{old}}(z \\mid x), \\theta) &= \\sum_{z} p_{\\theta_{\\mathrm{old}}}(z \\mid x) \\log  \\frac{p_{\\theta}(x, z)}{ \\log p_{\\theta_{\\mathrm{old}}}(z \\mid x)} \\\\\n",
    "&= \\mathbb{E}_{p_{\\theta_{\\mathrm{old}}}(z \\mid x)} \\left[ \\log \\frac{p_{\\theta}(x, z)}{p_{\\theta_{\\mathrm{old}}}(z \\mid x)} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "続いて、このELBOに対して$\\theta$の最適化を行うが、この式の形がsum-logの形になっているため解析的に解ける。これはMステップ(Maximization)と呼ばれる。\n",
    "\n",
    "以上のEステップとMステップを値が収束するまで繰り返す。\n",
    "### 複数データへの拡張\n",
    "$N$個の観測データ$x^{(1)}, x^{(2)}, \\ldots, x^{(N)}$に対してEMアルゴリズムを拡張する。各データに対応した任意の確率分布$q^{(1)}, q^{(2)}, \\ldots, q^{(N)}$を用意する。このとき対数尤度とELBOの関係は\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{n=1}^N \\log p_\\theta (x^{(n)}, z^{(n)})\n",
    "&\\geq \\sum_{n=1}^N \\mathrm{ELBO}(x^{(n)}; q^{(n)}, \\theta) \\\\\n",
    "&= \\sum_{n=1}^N \\sum_{z^{(n)}} q^{(n)}(z^{(n)}) \\log \\frac{p_\\theta(x^{(n)}, z^{(n)})}{q^{(n)}(z^{(n)})} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "となる。\n",
    "\n",
    "このとき、EMアルゴリズムは次のようにまとめられる。\n",
    "\n",
    "#### 1. Eステップ : $\\{ q^{(1)}, q^{(2)}, \\ldots, q^{(N)} \\}$の更新($\\theta$は固定)\n",
    "- 各$n$に対して、$ q^{(n)}(z) = p_\\theta (z | x^{(n)})$とする。\n",
    "#### 2. Mステップ : $\\theta$の更新($\\{ q^{(1)}, q^{(2)}, \\ldots, q^{(N)} \\}$は固定)\n",
    "- $\\sum_{n=1}^{N}{\\mathrm{ELBO}\\left(x^{(n)};q^{(n)},\\theta\\right)}$が最大に成る$\\theta$を解析的に求める\n",
    "#### 3. 終了判定 : 対数尤度の平均を計算して前回の対数尤度と比較する\n",
    "- $\\frac{1}{N}\\sum_{n=1}^{N}{\\log p\\left(x^{(n)};\\theta\\right)}$\n",
    "\n",
    "### EMアルゴリズムにおける対数尤度の単調増加性\n",
    "このEMアルゴリズムの1回の更新において次の対数尤度の単調増加性が成り立つ。\n",
    "$$\n",
    "\\log p\\left(x; \\theta_{\\mathrm{new}}\\right) \\geq \\log \\left(x;\\theta_{\\mathrm{old}}\\right)\n",
    "$$\n",
    "これを証明する。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " \\text{5.3.4} \\quad \\log p(x; \\theta_{\\text{new}}) \\geq \\log p(x; \\theta_{\\text{old}}) \\text{の証明} \\\\\n",
    " \\text{最後に} \\log p(x; \\theta_{\\text{new}}) \\geq \\log p(x; \\theta_{\\text{old}}) \\text{の証明を行います。まずはEステップにおいて} \\\\\n",
    " q_{\\text{old}}(z) = p_{\\theta_{\\text{old}}}(z | x) \\text{で更新したとき、対数尤度とELBOが一致するので次の式が成立します。} \\\\\n",
    " \\log p(x; \\theta_{\\text{old}}) = \\text{ELBO}(x; q_{\\text{old}}, \\theta_{\\text{old}}) \\\\\n",
    " \\text{Mステップでは上の式の右辺を最大化することで} \\theta_{\\text{new}} \\text{を得るので、次の式が成立します。} \\\\\n",
    " \\text{ELBO}(x; q_{\\text{old}}, \\theta_{\\text{new}}) \\geq \\text{ELBO}(x; q_{\\text{old}}, \\theta_{\\text{old}}) \\\\\n",
    " さらにその次のEステップでは、 q_{\\text{new}}(z) = p_{\\theta_{\\text{new}}}(z | x) \\text{で更新します。このとき次の式が成り立ちます。} \\\\\n",
    " \\log p(x; \\theta_{\\text{new}}) = \\text{ELBO}(x; q_{\\text{new}}, \\theta_{\\text{new}}) \\\\\n",
    " \\geq \\text{ELBO}(x; q_{\\text{old}}, \\theta_{\\text{new}}) \\\\\n",
    " \\text{以上を組み合わせることで、次の式が得られます。} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
